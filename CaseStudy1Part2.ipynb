{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request as request\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import argparse\n",
    "import boto3\n",
    "from botocore.client import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create logger with 'CaseStudy1Part2'\n",
    "logger = logging.getLogger('CaseStudy1Part2')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler('CaseStudy1Part2.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.ERROR)\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "# add the handlers to the logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: CaseStudy1Part2 [-h] [-y [YEAR]] [-d [DAY]] [-k AWSKEY]\n",
      "                       [-s AWSSECRETKEY] [-b AWSBUCKET]\n",
      "CaseStudy1Part2: error: unrecognized arguments: -f C:\\Users\\prshk\\AppData\\Roaming\\jupyter\\runtime\\kernel-28d6a3ca-77df-4ca2-889c-5f99dacbed0a.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('CaseStudy1Part2')\n",
    "parser.add_argument('-y','--year', help='The year for which the log file is to be downloaded.', type=str, default = '2005', nargs = '?')\n",
    "parser.add_argument('-d','--day', help='The day of each month for which the log file is to be downloaded.', type=str, default = '01', nargs = '?')\n",
    "parser.add_argument('-k','--awskey', help='The key to the aws account where the files is to be uploaded.', type=str)\n",
    "parser.add_argument('-s','--awssecretkey', help='The secret key to the aws account where the files is to be uploaded', type=str)\n",
    "parser.add_argument('-b','--awsbucket', help='The bucket where the files is to be uploaded', type=str)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yy = args.year\n",
    "dd = args.day\n",
    "awskey = args.awskey\n",
    "awssecretkey = args.awssecretkey\n",
    "awsbucket = args.awsbucket\n",
    "months = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "path = '.\\\\Part2Logs\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(path), exist_ok=True)\n",
    "for mm in months:\n",
    "    url = 'http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/' + yy + '/Qtr' + str(math.ceil(int(mm)/3.)) + '/log' + yy + mm + dd + '.zip'\n",
    "    request.urlretrieve(url,os.path.join(path,'log' + yy + mm + dd + '.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_name = '.\\\\Part2Logs'\n",
    "ext = '.zip'\n",
    "for item in os.listdir(dir_name): \n",
    "    if item.endswith(ext): \n",
    "        file_name = os.path.join(path,item) \n",
    "        zip_ref = zipfile.ZipFile(file_name)\n",
    "        zip_ref.extractall(dir_name) \n",
    "        zip_ref.close() \n",
    "        os.remove(file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_csv = pd.concat( [ pd.read_csv(path + item) for item in os.listdir(dir_name) if item.endswith('.csv') ] )\n",
    "for item in os.listdir(dir_name):\n",
    "    if item.endswith('.csv'):\n",
    "        os.remove(os.path.join(path,item))                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_missing_browser(df):\n",
    "    b_df = df[['ip','browser','cik']]\n",
    "    b_df = b_df[b_df['browser'].notnull()].drop_duplicates()\n",
    "    b_df = b_df.groupby(by=['ip', 'browser']).count().reset_index().sort_values(by=['cik'], ascending=False).groupby(by=['ip', 'browser']).head(1)\n",
    "    b_df = b_df[['ip', 'browser']]\n",
    "    df = pd.merge(df, b_df, on='ip', how='outer')   \n",
    "    df['browser_y'].fillna('Unknown', inplace=True)\n",
    "    df.drop(['browser_x'], axis=1, inplace=True)\n",
    "    df.rename(columns={'browser_y': 'browser'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_missing_size(df):\n",
    "    file_size_dict = df[['accession', 'extention', 'size']].drop_duplicates()[df['size'].notnull()]\n",
    "    for index, row in df.iterrows():\n",
    "        if row['size'] == np.NaN:\n",
    "            row_accession = row['accession']\n",
    "            row_extention = row['extention']\n",
    "            row['size'] = extention_size_dict.loc[(df['accession'] == row_accession) & (df['extention'] == row_extention)]\n",
    "    ext_size_dict = df[['extention', 'size']].drop_duplicates()[df['size'].notnull()].groupby('extention').mean()\n",
    "    for index, row in df.iterrows():\n",
    "        if row['size'] == np.NaN:\n",
    "            row_extention = row['extention']\n",
    "            row['size'] = extention_size_dict.loc[row_extention]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = combined_csv\n",
    "nan_df = df.apply(lambda x: sum(x.isnull()), axis=0).reset_index()\n",
    "nan_df.columns = ['Column_Name', 'No of NaNs']\n",
    "nan_df.to_csv(os.path.join(path,'missingdata.csv'))\n",
    "bdf = handle_missing_browser(df)\n",
    "edf = handle_missing_size(bdf)\n",
    "edf.to_csv(os.path.join(path,'combined.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edf['datetime'] = edf['date'].map(str) + ' ' + df['time'].map(str)\n",
    "edf['datetime'] = pd.to_datetime(edf['datetime'])\n",
    "edf.drop(['date', 'time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edf['quarter'] = edf['datetime'].dt.quarter\n",
    "edf['month'] = edf['datetime'].dt.month\n",
    "temp_df = edf[['cik', 'month', 'ip']]\n",
    "temp_df = temp_df.groupby(['month', 'cik']).count()\n",
    "temp_df = temp_df.reset_index()\n",
    "temp_df = temp_df.sort_values(by=['month', 'ip'], ascending=False).groupby('month').head(3)\n",
    "temp_df.to_csv(os.path.join(path,'cik-month.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf['hour'] = edf['datetime'].dt.hour\n",
    "conditions = [(edf['hour'] >= 18.0) & (edf['hour'] <= 23.0),(edf['hour'] <18.0) & (edf['hour'] >12.0),(edf['hour'] <=12.0)&(edf['hour'] >= 0.0)] \n",
    "choices = ['Night','Day','Morning']\n",
    "edf['tod'] = np.select(conditions, choices)\n",
    "temp_df = edf[['tod','ip']].groupby('tod').count().reset_index().sort_values(by=['ip'], ascending=False).head(3)\n",
    "temp_df.to_csv(os.path.join(path,'tod-ipcount.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shutil.make_archive('Part2logs', 'zip', path)\n",
    "shutil.rmtree(os.path.join('.\\\\Part2Logs')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Part2Logs.zip'\n",
    "data = open(file, 'rb')\n",
    "# S3 Connect\n",
    "s3 = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id=awskey,\n",
    "    aws_secret_access_key=awssecretkey,\n",
    "    config=Config(signature_version='s3v4')\n",
    ")\n",
    "# File Upload\n",
    "s3.Bucket(awsbucket).put_object(Key=file, Body=data, ACL='public-read-write')\n",
    "data.close()\n",
    "os.remove(os.path.join('.\\\\',file))\n",
    "print (\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
